## 1. Tá»•ng Quan Vá» Benchmark Trong Ceph.

Ceph lÃ  má»™t há»‡ thá»‘ng lÆ°u trá»¯ phÃ¢n tÃ¡n máº¡nh máº½, nhÆ°ng Ä‘á»ƒ Ä‘áº£m báº£o hiá»‡u suáº¥t tá»‘i Æ°u, chÃºng ta cáº§n thá»±c hiá»‡n benchmark Ä‘á»ƒ Ä‘o tá»‘c Ä‘á»™ Ä‘á»c/ghi, Ä‘á»™ trá»… cÅ©ng nhÆ° hiá»‡u suáº¥t cá»§a RBD (RADOS Block Device). Trong bÃ i viáº¿t nÃ y, chÃºng ta sáº½ tÃ¬m hiá»ƒu cÃ¡ch thá»±c hiá»‡n benchmark hiá»‡u suáº¥t Ceph chi tiáº¿t.


Benchmark lÃ  quÃ¡ trÃ¬nh Ä‘o lÆ°á»ng hiá»‡u suáº¥t cá»§a há»‡ thá»‘ng Ceph, bao gá»“m:

  + Tá»‘c Ä‘á»™ Ä‘á»c/ghi (Throughput & IOPS)
  + Äá»™ trá»… (Latency)
  + Hiá»‡u suáº¥t cá»§a RBD (Ceph Block Storage)

Viá»‡c benchmark giÃºp xÃ¡c Ä‘á»‹nh kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u, phÃ¡t hiá»‡n node tháº¯t cá»• chai (bottleneck), tá»‘i Æ°u cáº¥u hÃ¬nh OSD, MON, MDS vÃ  CRUSH.

## 2. CÃ¡c CÃ´ng Cá»¥ Benchmark Ceph.

CÃ³ nhiá»u cÃ´ng cá»¥ giÃºp Ä‘o hiá»‡u suáº¥t Ceph, bao gá»“m:

  + rados bench: Benchmark trá»±c tiáº¿p trÃªn pool Ceph
  + fio: Benchmark RBD
  + dd: Kiá»ƒm tra tá»‘c Ä‘á»™ ghi Ä‘Æ¡n giáº£n
  + ioping: Äo Ä‘á»™ trá»… I/O

## 3. Benchmark Tá»‘c Äá»™ Äá»c/Ghi Vá»›i RADOS

CÃ´ng cá»¥ rados bench Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ kiá»ƒm tra tá»‘c Ä‘á»™ Ä‘á»c/ghi cá»§a Ceph object storage.

#### 3.1 Benchmark ghi dá»¯ liá»‡u vÃ o Ceph

    ceph osd pool create testpool 128 128
    ceph osd pool application enable testpool rados
    rados bench -p testpool 60 write --no-cleanup

  + testpool: Pool benchmark
  + 60: Cháº¡y thá»­ nghiá»‡m trong 60 giÃ¢y
  + write: Kiá»ƒm tra tá»‘c Ä‘á»™ ghi
  + --no-cleanup: KhÃ´ng xÃ³a dá»¯ liá»‡u ngay sau thá»­ nghiá»‡m

Káº¿t quáº£ máº«u (hÃ¬nh áº£nh mang tÃ­nh cháº¥t minh há»a do mÃ¬nh dÃ¹ng áº£o hÃ³a 2 lá»›p Ä‘á»ƒ test) :

    Total writes made: 10240
    Bandwidth (MB/sec): 450.02
    Average latency: 0.055 sec

  <img src="cephimages/Screenshot_51.png">

#### 3.2 Benchmark Ä‘á»c dá»¯ liá»‡u tá»« Ceph

    rados bench -p testpool 60 seq

  + rados bench -p testpool 60 write --no-cleanupseq: Äá»c tuáº§n tá»±
  + Káº¿t quáº£ sáº½ thá»ƒ hiá»‡n tá»‘c Ä‘á»™ Ä‘á»c vÃ  Ä‘á»™ trá»….

#### 3.3 Benchmark Ä‘á»c ngáº«u nhiÃªn

    rados bench -p testpool 60 rand

  + rand: Äá»c ngáº«u nhiÃªn

## 4. Benchmark RBD (RADOS Block Device)

#### 4.1. Lá»±a chá»n tool benchmark.

RBD lÃ  má»™t trong nhá»¯ng á»©ng dá»¥ng phá»• biáº¿n cá»§a Ceph, thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong OpenStack, Proxmox, Kubernetes.

VÃ¬ pool POOL_LAB Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng cho RBD (RADOS Block Device), báº¡n khÃ´ng thá»ƒ sá»­ dá»¥ng rados bench Ä‘á»ƒ benchmark trá»±c tiáº¿p. LÃ½ do lÃ  rados bench hoáº¡t Ä‘á»™ng á»Ÿ má»©c object storage, trong khi RBD Ä‘Ã£ Ä‘á»‹nh dáº¡ng pool thÃ nh block storage.

Tuy nhiÃªn, báº¡n váº«n cÃ³ thá»ƒ benchmark pool báº±ng cÃ¡ch táº¡o má»™t RBD image vÃ  sá»­ dá»¥ng fio Ä‘á»ƒ kiá»ƒm tra hiá»‡u suáº¥t.

#### 4.2. Lá»±a chá»n client Ä‘á»ƒ map RBD.

Báº¡n cÃ³ thá»ƒ map RBD image vÃ o má»™t node trong cluster Ceph, nhÆ°ng khÃ´ng nÃªn map nÃ³ vÃ o Ceph MON vÃ¬:

  + Ceph MON (Monitor) chá»‰ quáº£n lÃ½ metadata, khÃ´ng cÃ³ chá»©c nÄƒng xá»­ lÃ½ dá»¯ liá»‡u RBD.
  + Hiá»‡u suáº¥t cÃ³ thá»ƒ bá»‹ áº£nh hÆ°á»Ÿng, vÃ¬ MON thÆ°á»ng khÃ´ng cÃ³ OSD, tá»©c lÃ  khÃ´ng trá»±c tiáº¿p lÆ°u trá»¯ dá»¯ liá»‡u.
  + Nguy cÆ¡ áº£nh hÆ°á»Ÿng cluster náº¿u MON bá»‹ quÃ¡ táº£i do I/O tá»« benchmark.

ğŸ‘‰ Giáº£i phÃ¡p há»£p lÃ½ hÆ¡n

Báº¡n nÃªn map RBD image trÃªn má»™t client bÃªn ngoÃ i Ceph, vÃ­ dá»¥:

  + Má»™t Proxmox node.

  + Má»™t VM hoáº·c Bare-metal server cÃ³ cÃ i Ceph client.

  + Má»™t Ceph OSD node (náº¿u khÃ´ng áº£nh hÆ°á»Ÿng hiá»‡u suáº¥t).

ğŸ“Œ CÃ¡ch map RBD trÃªn má»™t Ceph MON (hoáº·c client)

Náº¿u báº¡n váº«n muá»‘n thá»­ trÃªn MON (hoáº·c má»™t node báº¥t ká»³ trong cluster), báº¡n cáº§n:

  + CÃ i Ä‘áº·t Ceph client (náº¿u chÆ°a cÃ³).
  + XÃ¡c thá»±c vÃ  cáº¥u hÃ¬nh mÃ´i trÆ°á»ng.
  + Map RBD image.
  + Cháº¡y benchmark.

1ï¸âƒ£ CÃ i Ä‘áº·t Ceph client

Náº¿u node cá»§a báº¡n chÆ°a cÃ³ Ceph client, hÃ£y cÃ i Ä‘áº·t:

    apt update && apt install -y ceph ceph-common

(TrÃªn CentOS/RHEL: yum install -y ceph ceph-common)

2ï¸âƒ£ Kiá»ƒm tra quyá»n truy cáº­p

Cháº¡y lá»‡nh sau Ä‘á»ƒ Ä‘áº£m báº£o node cÃ³ thá»ƒ káº¿t ná»‘i vá»›i Ceph:

    ceph -s

Náº¿u tháº¥y tráº¡ng thÃ¡i HEALTH_OK hoáº·c HEALTH_WARN, nghÄ©a lÃ  káº¿t ná»‘i Ä‘Æ°á»£c.

3ï¸âƒ£ Map RBD image

ğŸ“Œ Láº¥y thÃ´ng tin keyring (náº¿u cáº§n): Náº¿u báº¡n dÃ¹ng user khÃ¡c admin, hÃ£y táº¡o keyring:

    ceph auth get-key client.admin | sudo tee /etc/ceph/ceph.client.admin.keyring

ğŸ“Œ Map RBD image:

    rbd map test-image --pool POOL_LAB

Map RBD image vÃ o há»‡ thá»‘ng

âœ… Báº¡n cÃ³ thá»ƒ map RBD vÃ o Ceph MON, nhÆ°ng khÃ´ng khuyáº¿n khÃ­ch vÃ¬ nÃ³ khÃ´ng Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ I/O.

âœ… NÃªn map RBD trÃªn má»™t Ceph client (Proxmox node, VM, Bare-metal server, hoáº·c Ceph OSD node).

âœ… Benchmark báº±ng fio trÃªn /dev/rbdX Ä‘á»ƒ Ä‘o tá»‘c Ä‘á»™ Ä‘á»c/ghi, Ä‘á»™ trá»… vÃ  IOPS.

Náº¿u báº¡n cáº§n test trá»±c tiáº¿p trÃªn MON, cá»© lÃ m theo hÆ°á»›ng dáº«n trÃªn. NhÆ°ng náº¿u cÃ³ client bÃªn ngoÃ i thÃ¬ tá»‘t hÆ¡n. ğŸš€

Map RBD image Ä‘á»ƒ há»‡ Ä‘iá»u hÃ nh nháº­n diá»‡n nÃ³ nhÆ° má»™t block device:

    shell> rbd map test-image --pool POOL_LAB
    /dev/rbd0

Kiá»ƒm tra thiáº¿t bá»‹ Ä‘Æ°á»£c gÃ¡n:

    shell> rbd showmapped
    id  pool           namespace  image       snap  device
    0   POOL_LAB             test-image  -     /dev/rbd0

VÃ­ dá»¥ Ä‘áº§u ra:

    shell> lsblk | grep ^rbd
    rbd0                      252:0    0   100G  0 disk

Thiáº¿t bá»‹ /dev/rbd0 sáº½ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ benchmark.

#### 4.1. Kiá»ƒm tra Pool vÃ  táº¡o Image RBD Ä‘á»ƒ benchmark

#### 4.2. Benchmark vá»›i fio ( Do lÃ  mÃ¡y test qua lá»›p áº£o hÃ³a nÃªn hÃ¬nh áº£nh chá»‰ mang tÃ­nh cháº¥t tÆ°á»£ng trÆ°ng

  + Benchmark tá»‘c Ä‘á»™ write throughput:

    fio --name=write_throughput --filename=/dev/rbd0 --numjobs=8 \
    --size=10G --time_based --runtime=60s --ramp_time=2s --ioengine=libaio \
    --direct=1 --verify=0 --bs=1M --iodepth=64 --rw=write \
    --group_reporting=1

  <img src="cephimages/Screenshot_52.png">

  + Benchmark tá»‘c Ä‘á»™ write iops:

    fio --name=write_iops --filename=/dev/rbd0 --size=10G \
    --time_based --runtime=60s --ramp_time=2s --ioengine=libaio --direct=1 --numjobs=8 \
    --verify=0 --bs=4K --iodepth=64 --rw=randwrite --group_reporting=1

  + Benchmark tá»‘c Ä‘á»™ read throughput:

    fio --name=read_throughput --filename=/dev/rbd0 --numjobs=8 \
    --size=10G --time_based --runtime=60s --ramp_time=2s --ioengine=libaio \
    --direct=1 --verify=0 --bs=1M --iodepth=64 --rw=read \
    --group_reporting=1

  <img src="cephimages/Screenshot_52.png">

  + Benchmark tá»‘c Ä‘á»™ read iops.

    fio --name=read_iops --filename=/dev/rbd0 --size=10G \
    --time_based --runtime=60s --ramp_time=2s --ioengine=libaio --direct=1 \
    --verify=0 --bs=4K --iodepth=64 --rw=randread --group_reporting=1
#### 5. OSD Latency: 

Kiá»ƒm tra báº±ng lá»‡nh:

    apt update && apt install -y ioping
    ioping -c 10 .