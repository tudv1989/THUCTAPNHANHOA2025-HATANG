## 1. Tổng quan.

Ceph Block Device (RBD) là một phần của hệ thống lưu trữ phân tán Ceph, cho phép nó hoạt động như một block device, giống như một ổ đĩa cứng vật lý.

RBD hỗ trợ việc chia nhỏ dữ liệu thành các block và phân phối chúng đều qua tất cả các OSD (Object Storage Daemons) trong cụm Ceph. Điều này giúp tăng hiệu suất và độ tin cậy, vì dữ liệu được lưu trữ trên nhiều OSD thay vì một ổ đĩa cứng duy nhất.

RBD cũng hỗ trợ snapshot và clone, cho phép bạn tạo bản sao của dữ liệu tại một thời điểm cụ thể và tạo các block device mới từ các snapshot đó.

RBDs có thể được ánh xạ vào hệ thống tệp cục bộ của bạn như một ổ đĩa cứng thông thường, và bạn có thể tương tác với chúng bằng các lệnh chuẩn như mount, df, v.v.


## 2. Sơ đồ LAB.

Cấu hình Ceph Client [ceph-client.dinhtu.xyz] để sử dụng Ceph Storage như sau.

  <img src="cephimages/Screenshot_28.png">

## 3. Tạo block device.

Ví dụ sau đây sẽ tạo một Block Device và mount nó trên Admin Node (ví dụ là ceph-node1.hoanghd.com).

#### Bước 1 – Cấu hình môi trường cho Ceph Client.

Để tiện thì mình sẽ chuyển khóa chung SSH sang cho Ceph Client và cấu hình nó từ Admin Node.

Hãy thêm nội dung mới vào file cấu hình SSH trong ~/.ssh/config để khai báo thông tin kết nối đến Ceph Client.

    cat >> ~/.ssh/config << 'OEF'
    Host ceph-client
        Hostname ceph-client.dinhtu.xyz ceph-client
        User root
    OEF

Bạn có thể xác minh lại nội dung file ~/.ssh/config sau khi thêm nội dung mới.

  <img src="cephimages/Screenshot_24.png">

Do mình thêm bằng shell làm quyền của file có thể bị thay đổi nên mình sẽ cấp lại quyền truy cập cho file cấu hình SSH để chỉ có người dùng hiện tại mới có thể đọc và ghi vào file.

    root@ceph-node1:~# chmod 600 ~/.ssh/config

Mình cũng trỏ file hosts về thông tin domain của Ceph Client cho Admin Node.

Mình cũng trỏ file hosts về thông tin domain của Ceph Client cho Admin Node.

    cat >> /etc/hosts << 'OEF'
    10.10.10.13 ceph-client.dinhtu.xyz ceph-client
    OEF

Xác minh lại thêm thông tin file hosts thành công.

    cat /etc/hosts

Giờ mình sẽ copy thông tin sshkey sang cho Ceph Client theo thông tin domain mình đã khai báo.

    ssh-copy-id -o StrictHostKeychecking=no ceph-client.dinhtu.xyz

Cài đặt Ceph Common cho Ceph Client.

    ssh ceph-client.dinhtu.xyz "apt update -y"

    ssh ceph-client.dinhtu.xyz "apt -y install ceph-common"

Copy file ceph.conf từ Admin Node sang Ceph Client.

    scp /etc/ceph/ceph.conf ceph-client.dinhtu.xyz:/etc/ceph/

  <img src="cephimages/Screenshot_25.png">

Copy ceph.client.admin.keyring từ Admin Node sang Ceph Client.

    scp /etc/ceph/ceph.client.admin.keyring ceph-client.dinhtu.xyz:/etc/ceph/

Phân quyền thư mục /etc/ceph/ cho Ceph Client.

    ssh ceph-client.dinhtu.xyz "chown ceph. /etc/ceph/ceph.*"

#### Bước 2 – Tạo một Block Device và mount nó vào Ceph Client.

Tạo RBD pool [rbd].

  + Lệnh ceph osd pool create rbd 128 được sử dụng để tạo một pool mới trong Ceph với tên là rbd và số lượng Placement Groups (PGs) là 128.

  + Trong Ceph, một pool là một tập hợp các đối tượng dữ liệu. Mỗi pool chứa một số lượng PGs, mà dữ liệu được chia nhỏ và phân phối đều qua tất cả các OSD (Object Storage Daemons) trong cụm.

    rbd trong lệnh này là tên của pool bạn muốn tạo, và 128 là số lượng PGs bạn muốn gán cho pool. Số lượng PGs cần phải được cân nhắc cẩn thận để tối ưu hóa hiệu suất và sử dụng lưu trữ.

    ssh ceph-client.dinhtu.xyz "ceph osd pool create rbd 128"

  <img src="cephimages/Screenshot_26.png">

Lệnh ceph osd pool set rbd pg_autoscale_mode on được sử dụng để bật chế độ tự động mở rộng PG (Placement Groups) cho pool rbd.

Trong Ceph, dữ liệu được chia thành các PG và được phân phối đều qua tất cả các OSD (Object Storage Daemons). Số lượng PG cần phải được cân nhắc cẩn thận để tối ưu hóa hiệu suất và sử dụng lưu trữ.

Chế độ pg_autoscale_mode cho phép Ceph tự động điều chỉnh số lượng PG dựa trên việc sử dụng lưu trữ thực tế. Khi bạn đặt pg_autoscale_mode thành on cho một pool, Ceph sẽ tự động tăng hoặc giảm số lượng PG cho pool đó dựa trên nhu cầu.

    ssh ceph-client.dinhtu.xyz "ceph osd pool set rbd pg_autoscale_mode on"

  <img src="cephimages/Screenshot_27.png">


Lệnh rbd pool init rbd được sử dụng để khởi tạo pool rbd cho việc sử dụng với RBD (RADOS Block Device).

RBD là một phần của Ceph, cho phép Ceph hoạt động như một block device, giống như một ổ đĩa cứng vật lý.

Khi bạn tạo một pool mới trong Ceph, bạn cần khởi tạo nó trước khi sử dụng với RBD. Lệnh rbd pool init thực hiện việc này, tạo ra các cấu trúc dữ liệu cần thiết trong pool để lưu trữ các block device.

Trong trường hợp này, rbd là tên của pool bạn muốn khởi tạo.

    ssh ceph-client.dinhtu.xyz "rbd pool init rbd"

Lệnh ceph osd pool autoscale-status được sử dụng để xem trạng thái tự động mở rộng của các pool trong cụm Ceph.

Ceph có khả năng tự động điều chỉnh số lượng PG (Placement Groups) cho mỗi pool dựa trên việc sử dụng lưu trữ thực tế, giúp tối ưu hóa hiệu suất và sử dụng lưu trữ.

Khi bạn chạy lệnh này, nó sẽ hiển thị danh sách các pool, trạng thái tự động mở rộng hiện tại của chúng (ON hoặc OFF), số lượng PG hiện tại và số lượng PG mà Ceph đề xuất dựa trên việc sử dụng lưu trữ.

    ssh ceph-client.dinhtu.xyz "ceph osd pool autoscale-status"

  <img src="cephimages/Screenshot_36.png">

Tạo một block device với dung lượng 10G.

    ssh ceph-client.dinhtu.xyz "rbd create --size 10G --pool rbd rbd01"

Xác minh lại kết quả sau khi tạo block device.

    ssh ceph-client.dinhtu.xyz "rbd ls -l"

  <img src="cephimages/Screenshot_37.png">

Lệnh rbd map rbd01 được sử dụng để ánh xạ một block device RBD (RADOS Block Device) tên là rbd01 vào hệ thống tệp cục bộ của bạn.

RBD là một phần của Ceph, cho phép Ceph hoạt động như một block device, giống như một ổ đĩa cứng vật lý. Khi bạn ánh xạ một RBD vào hệ thống của bạn, nó xuất hiện như một ổ đĩa cứng thông thường, và bạn có thể tương tác với nó bằng các lệnh chuẩn như mount, df, v.v.

Trong trường hợp này, rbd01 là tên của RBD bạn muốn ánh xạ.

    ssh ceph-client.dinhtu.xyz "rbd map rbd01"

  <img src="cephimages/Screenshot_38.png">

Lệnh rbd showmapped được sử dụng để hiển thị tất cả các block device RBD (RADOS Block Device) đã được ánh xạ vào hệ thống tệp cục bộ của bạn.

Khi bạn chạy lệnh này, nó sẽ hiển thị danh sách các RBD đã được ánh xạ, cùng với thông tin về ID ánh xạ, tên pool, tên RBD và thiết bị ánh xạ tương ứng trên hệ thống của bạn.

Điều này rất hữu ích khi bạn muốn xem thông tin về các RBD đã được ánh xạ, hoặc khi bạn muốn tìm thiết bị tương ứng trên hệ thống của bạn cho một RBD cụ thể.

    ssh ceph-client.dinhtu.xyz "rbd showmapped"

  <img src="cephimages/Screenshot_39.png">

Lúc này nếu bạn dùng lệnh lsblk để liệt kê danh sách các ổ đĩa trên Ceph Client bạn sẽ thấy kết quả có 1 block device với tên rbd0 có dung lượng 10G đã xuất hiện

    ssh ceph-client.dinhtu.xyz "lsblk"

  <img src="cephimages/Screenshot_40.png">

Lệnh mkfs.xfs /dev/rbd0 được sử dụng để tạo một hệ thống tệp XFS trên thiết bị /dev/rbd0.

Trong trường hợp này, /dev/rbd0 là một block device RBD (RADOS Block Device) đã được ánh xạ vào hệ thống tệp cục bộ của bạn.

XFS là một hệ thống tệp có khả năng mở rộng cao, được thiết kế để xử lý lượng dữ liệu lớn và là một lựa chọn tốt cho Ceph.

Khi bạn chạy lệnh này, nó sẽ tạo một hệ thống tệp XFS mới trên rbd0, làm cho nó sẵn sàng để lưu trữ dữ liệu.

    ssh ceph-client.dinhtu.xyz "mkfs.xfs /dev/rbd0"

  <img src="cephimages/Screenshot_41.png">

Sau khi định dạng xong bạn hãy mount rbd0 vào thư mục nào đó để sử dụng nhé.

    ssh ceph-client.dinhtu.xyz "mount /dev/rbd0 /mnt"

Và đây là kết quả của df -h sau khi mount.

    ssh ceph-client.dinhtu.xyz "df -h"

  <img src="cephimages/Screenshot_42.png">

## 4. Xóa block device.

Để xóa block device hoặc pools bạn đã tạo, thực hiện theo quy trình sau:

#### Bước 1 – Hãy list các block device hiện có của bạn và chọn block device bạn muốn xóa.

Mình sẽ remote vào ceph-client và chạy lệnh ls /dev/rbd/rbd/ để list các block device hiện có.








